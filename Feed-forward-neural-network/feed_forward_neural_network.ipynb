{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGeJrSD6dQa4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A feed forward neural network** (FFNN) is a type of artificial neural network where the information flows in one direction, from the input layer through one or more hidden layers to the output layer. It is called \"feed-forward\" because the data moves forward through the network without any loops or feedback connections.\n",
        "![download (3).jpeg](data:image/jpeg;base64,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)\n",
        "\n",
        "The FFNN is composed of multiple interconnected layers of nodes, also known as neurons. Each neuron in a layer is connected to all neurons in the previous layer and all neurons in the subsequent layer, forming a directed acyclic graph structure. The connections between neurons are represented by weights, which are adjustable parameters that determine the strength and significance of the connection.\n",
        "\n",
        "The network receives input data, which is propagated through the layers, and eventually produces an output at the final layer. Each neuron in the hidden layers applies a nonlinear activation function to the weighted sum of its inputs, which introduces nonlinearity and allows the network to learn complex patterns and relationships in the data.\n",
        "\n",
        "The primary purpose of a feed-forward neural network is to approximate a function that maps the input data to the desired output. This is achieved through a training process called backpropagation, where the network adjusts the weights based on the error between the predicted output and the actual output. The objective is to minimize this error by iteratively updating the weights using gradient descent optimization or related algorithms.\n",
        "\n",
        "FFNNs can be used for a variety of tasks, including classification, regression, pattern recognition, and function approximation. They have been successfully applied in various domains, such as image and speech recognition, natural language processing, and financial forecasting.\n",
        "\n",
        "It's worth noting that the concept of a feed-forward neural network predates more advanced architectures like recurrent neural networks (RNNs) or convolutional neural networks (CNNs), which incorporate feedback connections or specific weight-sharing schemes, respectively, to address different types of problems. However, feed-forward neural networks remain a fundamental and widely used type of neural network in the field of machine learning."
      ],
      "metadata": {
        "id": "XcPm9aKydmKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **feedforward neural network** (FNN) is a type of artificial neural network in which the connections between the nodes do not form a cycle. This means that the information flows in only one direction, from the input layer to the output layer. FNNs are one of the simplest types of neural networks, and they are often used as a starting point for learning about neural networks.\n",
        "\n",
        "FNNs are made up of a series of layers, each of which contains a number of neurons. The input layer is the first layer of the network, and it receives the input data. The output layer is the last layer of the network, and it produces the output of the network. The hidden layers are the layers in between the input layer and the output layer.\n",
        "\n",
        "Each neuron in an FNN has a number of inputs, and it produces an output. The inputs to a neuron are the outputs of the neurons in the previous layer. The output of a neuron is calculated using a function called an **activation function**. The activation function determines how the input to a neuron is converted into an output.\n",
        "\n",
        "The weights of the connections between the neurons are the parameters that are learned by the FNN. The weights are adjusted during training so that the FNN can produce the desired output for a given input.\n",
        "\n",
        "FNNs can be used to solve a variety of problems, including classification, regression, and time series forecasting. They are a powerful tool for machine learning, and they are used in a wide variety of applications.\n",
        "\n",
        "Here are some of the **advantages** of using feedforward neural networks:\n",
        "They are relatively simple to understand and implement.\n",
        "They can be trained to solve a variety of problems.\n",
        "They are able to learn complex relationships between input and output data.\n",
        "They are able to generalize to new data that was not seen during training.\n",
        "\n",
        "Here are some of the **disadvantages** of using feedforward neural networks:\n",
        "They can be computationally expensive to train.\n",
        "They can be sensitive to the choice of hyperparameters.\n",
        "They can be difficult to interpret.\n"
      ],
      "metadata": {
        "id": "9slZc4UPjkaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the number of neurons in each layer\n",
        "input_size = 784\n",
        "hidden_size = 100\n",
        "output_size = 10\n",
        "\n",
        "# Initialize the weights and biases\n",
        "weights = np.random.randn(input_size, hidden_size)\n",
        "biases = np.random.randn(hidden_size)\n",
        "\n",
        "# Define the activation function\n",
        "activation_function = lambda x: np.tanh(x)\n",
        "\n",
        "# Define the loss function\n",
        "loss_function = lambda y_hat, y: np.mean((y_hat - y)**2)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(100):\n",
        "    # Get a batch of data\n",
        "    x_batch, y_batch = get_batch()\n",
        "\n",
        "    # Forward pass\n",
        "    y_hat = activation_function(np.dot(x_batch, weights) + biases)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_function(y_hat, y_batch)\n",
        "\n",
        "    # Backpropagate the error\n",
        "    dloss_dy_hat = -(y_batch - y_hat)\n",
        "    ddy_dx = activation_function(np.dot(x_batch, weights) + biases)\n",
        "    dloss_dx = np.dot(dloss_dy_hat, ddy_dx)\n",
        "\n",
        "    # Update the weights and biases\n",
        "    weights -= optimizer.gradient(loss, weights)\n",
        "    biases -= optimizer.gradient(loss, biases)\n",
        "\n",
        "# Evaluate the network\n",
        "y_pred = activation_function(np.dot(x_test, weights) + biases)\n",
        "accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "r0NwdXPXe-Y9",
        "outputId": "37690459-8a11-44e0-dd98-00835c85e002"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c5cfb5f1a712>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Get a batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_batch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the number of neurons in each layer\n",
        "input_size = 784\n",
        "hidden_size = 100\n",
        "output_size = 10\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(hidden_size, activation='tanh', input_shape=(input_size,)),\n",
        "    tf.keras.layers.Dense(output_size, activation='linear')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
        "\n",
        "def get_batch(batch_size):\n",
        "    # Get a random batch of data from the MNIST dataset\n",
        "    x_batch, y_batch = tf.keras.datasets.mnist.train.next_batch(batch_size)\n",
        "    return x_batch, y_batch\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(100):\n",
        "    # Get a batch of data\n",
        "    x_batch, y_batch = get_batch(1000)\n",
        "\n",
        "    # Train on the batch\n",
        "    model.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "# Evaluate the network\n",
        "y_pred = model.predict(x_test)\n",
        "accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0jeRHqI9fFak",
        "outputId": "5eb26d31-892e-41f2-db78-042eae4571c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7f8f44cedca8>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Get a batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Train on the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7f8f44cedca8>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Get a random batch of data from the MNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.datasets.mnist' has no attribute 'train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's correct! Feed-forward neural networks (FFNNs) are versatile and can be applied to various tasks and domains. Here are some examples of how FFNNs have been used:\n",
        "\n",
        "1. Classification: FFNNs can be used for tasks where the goal is to assign inputs to specific classes or categories. For instance, they have been applied to image classification problems, where the network learns to classify images into different classes, such as recognizing objects or identifying handwritten digits. FFNNs have also been used for text classification tasks, such as sentiment analysis or spam detection.\n",
        "\n",
        "2. Regression: FFNNs can be used for tasks where the goal is to predict a continuous numerical value. For example, they can be used for predicting housing prices based on various features like location, size, and amenities. FFNNs can also be used for time series forecasting, where they predict future values based on historical data.\n",
        "\n",
        "3. Pattern recognition: FFNNs excel at learning and recognizing patterns in data. They have been successfully used in computer vision tasks like object detection, where the network learns to detect and locate objects within images. FFNNs have also been employed for speech recognition tasks, where they learn to transcribe spoken words into written text.\n",
        "\n",
        "4. Function approximation: FFNNs can be used to approximate complex mathematical functions. They can learn the underlying relationships between inputs and outputs, allowing them to approximate functions that may not have a straightforward analytical form. This makes them useful in scientific and engineering domains where complex relationships need to be modeled.\n",
        "\n",
        "5. Natural language processing (NLP): FFNNs have been applied to various NLP tasks, such as language translation, sentiment analysis, text generation, and named entity recognition. They can learn to understand and process human language by mapping input text to specific outputs or generating coherent text sequences.\n",
        "\n",
        "6. Financial forecasting: FFNNs have been used in financial applications, such as stock market prediction, credit risk assessment, and algorithmic trading. They can learn from historical data to identify patterns and trends that may influence future financial outcomes.\n",
        "\n",
        "Overall, FFNNs have proven to be powerful tools in many domains, thanks to their ability to learn complex relationships and make predictions or classifications based on input data."
      ],
      "metadata": {
        "id": "jOvlDeqeng9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Generate some sample data for binary classification\n",
        "# X represents input features, y represents class labels\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=1)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "rounded_predictions = np.round(predictions)\n",
        "\n",
        "# Print the predictions\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]}, Predicted: {rounded_predictions[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnfkFX6YnlZt",
        "outputId": "3ee03d55-7da7-44ae-e55a-3b7421e60b20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 0.7008 - accuracy: 0.5000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6996 - accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6989 - accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6982 - accuracy: 0.7500\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6984 - accuracy: 0.5000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6969 - accuracy: 0.5000\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6962 - accuracy: 0.5000\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6953 - accuracy: 0.5000\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5000\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6942 - accuracy: 0.5000\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5000\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5000\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5000\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5000\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5000\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5000\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5000\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5000\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5000\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5000\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5000\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5000\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5000\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.5000\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6836 - accuracy: 0.5000\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6829 - accuracy: 0.5000\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6827 - accuracy: 0.5000\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6818 - accuracy: 0.5000\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.5000\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6804 - accuracy: 0.5000\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.5000\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6794 - accuracy: 0.5000\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6785 - accuracy: 0.5000\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6782 - accuracy: 0.5000\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.5000\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.5000\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6768 - accuracy: 0.5000\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6760 - accuracy: 0.5000\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.5000\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6745 - accuracy: 0.5000\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6737 - accuracy: 0.5000\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6735 - accuracy: 0.5000\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.5000\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.5000\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6718 - accuracy: 0.5000\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6714 - accuracy: 0.5000\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.5000\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6707 - accuracy: 0.5000\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6706 - accuracy: 0.5000\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6705 - accuracy: 0.5000\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6696 - accuracy: 0.5000\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6693 - accuracy: 0.5000\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.5000\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6687 - accuracy: 0.5000\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6685 - accuracy: 0.5000\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6679 - accuracy: 0.5000\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6679 - accuracy: 0.5000\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6675 - accuracy: 0.5000\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.5000\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6672 - accuracy: 0.5000\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6667 - accuracy: 0.5000\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6661 - accuracy: 0.5000\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6658 - accuracy: 0.5000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6654 - accuracy: 0.5000\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6652 - accuracy: 0.5000\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6649 - accuracy: 0.5000\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6647 - accuracy: 0.5000\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.5000\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.5000\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6636 - accuracy: 0.5000\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.5000\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6634 - accuracy: 0.5000\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6626 - accuracy: 0.5000\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6626 - accuracy: 0.5000\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6621 - accuracy: 0.5000\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.5000\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6615 - accuracy: 0.5000\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6612 - accuracy: 0.5000\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6608 - accuracy: 0.5000\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6606 - accuracy: 0.5000\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.5000\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6603 - accuracy: 0.5000\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6596 - accuracy: 0.5000\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6593 - accuracy: 0.5000\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6591 - accuracy: 0.5000\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.5000\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6585 - accuracy: 0.5000\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6582 - accuracy: 0.5000\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6579 - accuracy: 0.5000\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6575 - accuracy: 0.5000\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6577 - accuracy: 0.5000\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.5000\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6568 - accuracy: 0.5000\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6564 - accuracy: 0.5000\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6561 - accuracy: 0.5000\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.5000\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6560 - accuracy: 0.5000\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6553 - accuracy: 0.5000\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6549 - accuracy: 0.5000\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6547 - accuracy: 0.5000\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "Input: [0 0], Predicted: [0.]\n",
            "Input: [0 1], Predicted: [1.]\n",
            "Input: [1 0], Predicted: [0.]\n",
            "Input: [1 1], Predicted: [1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Generate some sample data for regression\n",
        "# X represents input features, y represents target values (housing prices)\n",
        "X = np.array([[1.0, 2.0, 3.0], [2.0, 3.0, 4.0], [3.0, 4.0, 5.0], [4.0, 5.0, 6.0]])\n",
        "y = np.array([100000, 150000, 200000, 250000])\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=3, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=1)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Print the predictions\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]}, Target: {y[i]}, Predicted: {predictions[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAOTgr9knqL2",
        "outputId": "1782f53b-f960-4cd2-cd22-50ab50e10104"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33750192128.0000\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33750147072.0000\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33750106112.0000\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33750063104.0000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33750011904.0000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749972992.0000\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749929984.0000\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749889024.0000\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749852160.0000\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749807104.0000\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749768192.0000\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749725184.0000\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749688320.0000\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749649408.0000\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749606400.0000\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749569536.0000\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749538816.0000\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749497856.0000\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749460992.0000\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749432320.0000\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749393408.0000\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749368832.0000\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749331968.0000\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749299200.0000\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749272576.0000\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749239808.0000\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749209088.0000\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749174272.0000\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749143552.0000\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749114880.0000\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33749084160.0000\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749059584.0000\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749028864.0000\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33749000192.0000\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748973568.0000\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748951040.0000\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748928512.0000\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748903936.0000\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748881408.0000\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748856832.0000\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748832256.0000\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748807680.0000\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748785152.0000\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748762624.0000\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748733952.0000\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748711424.0000\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748690944.0000\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748664320.0000\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33748643840.0000\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748611072.0000\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748584448.0000\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748561920.0000\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748545536.0000\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748514816.0000\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748484096.0000\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748459520.0000\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33748430848.0000\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748404224.0000\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748375552.0000\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748350976.0000\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748320256.0000\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33748293632.0000\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748260864.0000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748236288.0000\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748205568.0000\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748172800.0000\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748135936.0000\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748115456.0000\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748074496.0000\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748045824.0000\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33748017152.0000\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33747980288.0000\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747951616.0000\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33747914752.0000\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747879936.0000\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33747841024.0000\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747804160.0000\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747771392.0000\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747740672.0000\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747701760.0000\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747656704.0000\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747615744.0000\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747589120.0000\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747546112.0000\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747496960.0000\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747456000.0000\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747419136.0000\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747380224.0000\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747335168.0000\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33747292160.0000\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 33747238912.0000\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33747197952.0000\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33747156992.0000\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747113984.0000\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747068928.0000\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33747011584.0000\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33746964480.0000\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 33746919424.0000\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33746880512.0000\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 33746827264.0000\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Input: [1. 2. 3.], Target: 100000, Predicted: [4.946674]\n",
            "Input: [2. 3. 4.], Target: 150000, Predicted: [7.2266173]\n",
            "Input: [3. 4. 5.], Target: 200000, Predicted: [9.50656]\n",
            "Input: [4. 5. 6.], Target: 250000, Predicted: [11.786504]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#time series forecasting\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Generate some sample time series data for forecasting\n",
        "# X represents input features, y represents target values\n",
        "X = np.array([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n",
        "y = np.array([6, 7, 8])\n",
        "\n",
        "# Reshape the input data to match the expected input shape of the model\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=1)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Print the predictions\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i].flatten()}, Target: {y[i]}, Predicted: {predictions[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb0dLJdunybq",
        "outputId": "ab0d18b0-813b-4433-f331-0fd3df6a4790"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3/3 [==============================] - 1s 4ms/step - loss: 33.5717\n",
            "Epoch 2/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 33.1142\n",
            "Epoch 3/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 32.6485\n",
            "Epoch 4/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 32.1724\n",
            "Epoch 5/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 31.6882\n",
            "Epoch 6/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 31.2736\n",
            "Epoch 7/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 30.8241\n",
            "Epoch 8/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 30.3955\n",
            "Epoch 9/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 29.9108\n",
            "Epoch 10/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 29.5280\n",
            "Epoch 11/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 29.0458\n",
            "Epoch 12/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 28.6929\n",
            "Epoch 13/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 28.2304\n",
            "Epoch 14/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 27.8440\n",
            "Epoch 15/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 27.4718\n",
            "Epoch 16/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 27.0849\n",
            "Epoch 17/100\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 26.6736\n",
            "Epoch 18/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 26.2826\n",
            "Epoch 19/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 25.8948\n",
            "Epoch 20/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 25.5100\n",
            "Epoch 21/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 25.1079\n",
            "Epoch 22/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 24.7740\n",
            "Epoch 23/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 24.3991\n",
            "Epoch 24/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 24.0335\n",
            "Epoch 25/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 23.6414\n",
            "Epoch 26/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 23.3372\n",
            "Epoch 27/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 22.9862\n",
            "Epoch 28/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 22.6400\n",
            "Epoch 29/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 22.3232\n",
            "Epoch 30/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 22.0258\n",
            "Epoch 31/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 21.6874\n",
            "Epoch 32/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 21.3770\n",
            "Epoch 33/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 21.0815\n",
            "Epoch 34/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 20.7455\n",
            "Epoch 35/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 20.4553\n",
            "Epoch 36/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 20.1408\n",
            "Epoch 37/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 19.8119\n",
            "Epoch 38/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 19.5043\n",
            "Epoch 39/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 19.2186\n",
            "Epoch 40/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 18.8842\n",
            "Epoch 41/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 18.5948\n",
            "Epoch 42/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 18.2844\n",
            "Epoch 43/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 18.0228\n",
            "Epoch 44/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 17.7097\n",
            "Epoch 45/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 17.3937\n",
            "Epoch 46/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 17.0958\n",
            "Epoch 47/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 16.8337\n",
            "Epoch 48/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 16.5241\n",
            "Epoch 49/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 16.2416\n",
            "Epoch 50/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 15.9590\n",
            "Epoch 51/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 15.6773\n",
            "Epoch 52/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 15.4018\n",
            "Epoch 53/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 15.1416\n",
            "Epoch 54/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 14.8430\n",
            "Epoch 55/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 14.5851\n",
            "Epoch 56/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 14.3191\n",
            "Epoch 57/100\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 14.0549\n",
            "Epoch 58/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 13.7969\n",
            "Epoch 59/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 13.5540\n",
            "Epoch 60/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 13.2839\n",
            "Epoch 61/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 13.0338\n",
            "Epoch 62/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 12.7826\n",
            "Epoch 63/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 12.5480\n",
            "Epoch 64/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 12.3031\n",
            "Epoch 65/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 12.0748\n",
            "Epoch 66/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 11.8437\n",
            "Epoch 67/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 11.6267\n",
            "Epoch 68/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 11.3863\n",
            "Epoch 69/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 11.1848\n",
            "Epoch 70/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 10.9460\n",
            "Epoch 71/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 10.7464\n",
            "Epoch 72/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 10.5354\n",
            "Epoch 73/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 10.3180\n",
            "Epoch 74/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 10.1189\n",
            "Epoch 75/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 9.9389\n",
            "Epoch 76/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 9.7274\n",
            "Epoch 77/100\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 9.5408\n",
            "Epoch 78/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 9.3562\n",
            "Epoch 79/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 9.1900\n",
            "Epoch 80/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 9.0098\n",
            "Epoch 81/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 8.8302\n",
            "Epoch 82/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 8.6604\n",
            "Epoch 83/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 8.4992\n",
            "Epoch 84/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 8.3251\n",
            "Epoch 85/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 8.1779\n",
            "Epoch 86/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 8.0243\n",
            "Epoch 87/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 7.8676\n",
            "Epoch 88/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 7.7210\n",
            "Epoch 89/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 7.5818\n",
            "Epoch 90/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 7.4487\n",
            "Epoch 91/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 7.3244\n",
            "Epoch 92/100\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 7.1892\n",
            "Epoch 93/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 7.0564\n",
            "Epoch 94/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 6.9348\n",
            "Epoch 95/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 6.8243\n",
            "Epoch 96/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 6.7001\n",
            "Epoch 97/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 6.5893\n",
            "Epoch 98/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 6.4863\n",
            "Epoch 99/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 6.3764\n",
            "Epoch 100/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 6.2817\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "Input: [1 2 3 4 5], Target: 6, Predicted: [[1.7979248]\n",
            " [2.8746898]\n",
            " [3.9514549]\n",
            " [5.0282197]\n",
            " [6.1049848]]\n",
            "Input: [2 3 4 5 6], Target: 7, Predicted: [[2.8746898]\n",
            " [3.9514549]\n",
            " [5.0282197]\n",
            " [6.1049848]\n",
            " [7.1817503]]\n",
            "Input: [3 4 5 6 7], Target: 8, Predicted: [[3.9514549]\n",
            " [5.0282197]\n",
            " [6.1049848]\n",
            " [7.1817503]\n",
            " [8.258515 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Generate some sample data for object detection\n",
        "# X represents input images, y represents object labels\n",
        "X = np.random.random((100, 100))  # Sample input images\n",
        "y = np.random.randint(2, size=(100, 1))  # Sample object labels (0 or 1)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=10000))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10, batch_size=16)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Print the predictions\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]}, Target: {y[i]}, Predicted: {predictions[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "fFEoE4b8oFFI",
        "outputId": "94db4321-528e-40d3-d8f8-345522630775"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5a37353f06eb>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 10000), found shape=(None, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Generate some sample data for speech recognition\n",
        "# X represents input audio features, y represents transcriptions\n",
        "X = np.random.random((100, 10))  # Sample audio features\n",
        "y = np.random.randint(10, size=(100, 1))  # Sample transcriptions\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=10))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10, batch_size=16)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Print the predictions\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]}, Target: {y[i]}, Predicted: {np.argmax(predictions[i])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npuslr96oO09",
        "outputId": "58a7729d-d3d6-4700-a5ab-acb045398da5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 3ms/step - loss: 2.2973 - accuracy: 0.1600\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 2.2804 - accuracy: 0.1600\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.2707 - accuracy: 0.1400\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.2620 - accuracy: 0.1300\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.2557 - accuracy: 0.1600\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.2496 - accuracy: 0.1900\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.2452 - accuracy: 0.1900\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.2388 - accuracy: 0.1900\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.2327 - accuracy: 0.1800\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 2.2271 - accuracy: 0.1800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f193ed16050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "Input: [0.07381233 0.42633542 0.99627825 0.11098546 0.38569169 0.99685949\n",
            " 0.93796565 0.13914872 0.83757    0.80286524], Target: [0], Predicted: 2\n",
            "Input: [0.43964929 0.4948322  0.9305747  0.55134825 0.37389943 0.84946246\n",
            " 0.60785241 0.11927834 0.52603423 0.13183117], Target: [0], Predicted: 2\n",
            "Input: [0.75571392 0.98330804 0.60866073 0.67868188 0.33171809 0.28224412\n",
            " 0.79079414 0.88739204 0.92116994 0.52472693], Target: [1], Predicted: 2\n",
            "Input: [0.69045816 0.21235921 0.01939185 0.66320926 0.92728604 0.54435186\n",
            " 0.15995265 0.35401142 0.32003885 0.12853795], Target: [4], Predicted: 2\n",
            "Input: [0.18230514 0.49910532 0.20907282 0.22509012 0.40875279 0.09423685\n",
            " 0.93021548 0.29805894 0.56375863 0.265729  ], Target: [0], Predicted: 2\n",
            "Input: [0.15163584 0.90194873 0.69860188 0.99346438 0.93009596 0.10789626\n",
            " 0.68660061 0.66323096 0.14732049 0.62044199], Target: [8], Predicted: 2\n",
            "Input: [0.74971053 0.11821056 0.43250635 0.57722851 0.6502426  0.92644082\n",
            " 0.92798579 0.47136286 0.99152784 0.02055551], Target: [1], Predicted: 2\n",
            "Input: [0.72814633 0.9908061  0.49714458 0.2647515  0.58270568 0.51907795\n",
            " 0.7052236  0.21183696 0.71781062 0.04790938], Target: [3], Predicted: 2\n",
            "Input: [0.14831784 0.02441231 0.59585261 0.64716918 0.2066128  0.63867454\n",
            " 0.24893118 0.20614336 0.80281538 0.12242564], Target: [6], Predicted: 4\n",
            "Input: [0.21782619 0.9031829  0.72432812 0.10038006 0.92870766 0.91972905\n",
            " 0.11783022 0.12337506 0.15990867 0.88909951], Target: [4], Predicted: 2\n",
            "Input: [0.17887824 0.17896101 0.93582541 0.79932502 0.41878356 0.72560445\n",
            " 0.63280119 0.99454746 0.62978489 0.96481759], Target: [2], Predicted: 2\n",
            "Input: [0.63775928 0.23851434 0.665706   0.28753295 0.40594958 0.69180807\n",
            " 0.04172205 0.02949815 0.96691798 0.48906539], Target: [2], Predicted: 4\n",
            "Input: [0.49807019 0.33045751 0.30595521 0.07173732 0.37650673 0.23058746\n",
            " 0.09422481 0.39825691 0.55441602 0.95107285], Target: [2], Predicted: 2\n",
            "Input: [0.73640276 0.10692113 0.69001323 0.44920126 0.50106181 0.50581243\n",
            " 0.3747814  0.40280192 0.9725012  0.050531  ], Target: [0], Predicted: 4\n",
            "Input: [0.66860833 0.9126455  0.859617   0.50482606 0.92565467 0.3753887\n",
            " 0.41373761 0.63048816 0.56993264 0.77533524], Target: [7], Predicted: 2\n",
            "Input: [0.44808842 0.06520863 0.28167219 0.23820009 0.60677738 0.03084948\n",
            " 0.5197096  0.52524965 0.75373325 0.83449193], Target: [8], Predicted: 2\n",
            "Input: [0.76998328 0.01947257 0.36112791 0.89614019 0.27337596 0.06090789\n",
            " 0.55984839 0.92462286 0.30008732 0.67832733], Target: [3], Predicted: 3\n",
            "Input: [0.32775665 0.70271178 0.8813946  0.71615257 0.99776984 0.89358096\n",
            " 0.39222152 0.81896013 0.57629103 0.60846565], Target: [3], Predicted: 2\n",
            "Input: [0.19467655 0.1776715  0.23470356 0.01943932 0.44776308 0.29169914\n",
            " 0.13924281 0.43912735 0.12773923 0.83331879], Target: [5], Predicted: 2\n",
            "Input: [0.23546766 0.31086074 0.11676682 0.66734436 0.3803535  0.44900723\n",
            " 0.23026771 0.26607591 0.43711199 0.73465623], Target: [6], Predicted: 2\n",
            "Input: [0.60083094 0.92988653 0.23310404 0.13332318 0.26107674 0.12087002\n",
            " 0.82809554 0.20407912 0.58615899 0.77300251], Target: [2], Predicted: 2\n",
            "Input: [6.58081858e-01 2.92221276e-01 7.44042363e-01 3.42282167e-01\n",
            " 4.69370398e-02 9.62014808e-01 4.08720936e-04 7.91323941e-01\n",
            " 1.64223233e-01 8.45338225e-01], Target: [5], Predicted: 4\n",
            "Input: [0.32858101 0.44638013 0.21935692 0.33928518 0.01591717 0.13250254\n",
            " 0.68214098 0.0735436  0.96846332 0.78599379], Target: [6], Predicted: 2\n",
            "Input: [0.80316401 0.22517044 0.31614792 0.38404354 0.17465837 0.13793628\n",
            " 0.2367842  0.36381101 0.24571686 0.2436455 ], Target: [5], Predicted: 4\n",
            "Input: [0.04438589 0.63506365 0.47771311 0.29314695 0.03325515 0.14216751\n",
            " 0.09264436 0.74238133 0.49787605 0.63165924], Target: [7], Predicted: 2\n",
            "Input: [0.64409265 0.43498466 0.28441231 0.99212472 0.26388041 0.63415624\n",
            " 0.98174426 0.23461895 0.01121661 0.75191113], Target: [0], Predicted: 2\n",
            "Input: [0.35897823 0.92812825 0.19254067 0.7109014  0.608788   0.22591961\n",
            " 0.13294758 0.86790029 0.17210267 0.96866486], Target: [8], Predicted: 2\n",
            "Input: [0.63076818 0.5882761  0.20751604 0.30093175 0.3653434  0.08019768\n",
            " 0.46603249 0.65280354 0.66922219 0.71679386], Target: [0], Predicted: 2\n",
            "Input: [0.53560327 0.56744589 0.57330584 0.33160377 0.53516858 0.96033682\n",
            " 0.35984997 0.72166382 0.92946044 0.84713759], Target: [5], Predicted: 2\n",
            "Input: [0.03034823 0.02517243 0.46085477 0.3251029  0.18668988 0.99567021\n",
            " 0.37950125 0.49869687 0.34752403 0.77491447], Target: [5], Predicted: 8\n",
            "Input: [0.33676717 0.23812698 0.97453753 0.9030193  0.92629325 0.68668147\n",
            " 0.44487341 0.79966469 0.4647135  0.63485884], Target: [2], Predicted: 2\n",
            "Input: [0.00559759 0.17433553 0.09133636 0.58970892 0.70846271 0.73042951\n",
            " 0.83599942 0.094832   0.46645133 0.08947219], Target: [2], Predicted: 2\n",
            "Input: [0.70421484 0.39032573 0.97266724 0.99883213 0.26214137 0.55619042\n",
            " 0.84433839 0.07690028 0.36952519 0.76624809], Target: [1], Predicted: 2\n",
            "Input: [0.55301034 0.52351502 0.76876128 0.40991392 0.0309695  0.32359425\n",
            " 0.12804489 0.79332145 0.28023206 0.98352029], Target: [6], Predicted: 4\n",
            "Input: [0.6522735  0.50508291 0.24695348 0.44185028 0.37215352 0.86742343\n",
            " 0.57503432 0.47068334 0.44892454 0.95498751], Target: [4], Predicted: 2\n",
            "Input: [0.69666999 0.6571191  0.7901535  0.95899732 0.79120725 0.67974348\n",
            " 0.82577918 0.08627136 0.89664373 0.48983131], Target: [0], Predicted: 2\n",
            "Input: [0.83339818 0.02511472 0.16384212 0.38374763 0.00711513 0.4405044\n",
            " 0.82714437 0.76820073 0.47520517 0.74004771], Target: [3], Predicted: 4\n",
            "Input: [0.0656799  0.92786525 0.07485276 0.42558711 0.66335258 0.40635505\n",
            " 0.60193947 0.8394017  0.8740316  0.28442929], Target: [5], Predicted: 2\n",
            "Input: [0.31651456 0.06708252 0.1943198  0.53881496 0.79558277 0.38219503\n",
            " 0.81852373 0.31198446 0.14900514 0.58658703], Target: [4], Predicted: 2\n",
            "Input: [0.33499325 0.39365702 0.54022434 0.69437539 0.36182319 0.07797546\n",
            " 0.50812162 0.14048486 0.31758871 0.89789847], Target: [5], Predicted: 2\n",
            "Input: [0.62723944 0.95341488 0.5479249  0.1493078  0.96661267 0.7774238\n",
            " 0.09195196 0.28160974 0.05303746 0.64877916], Target: [1], Predicted: 2\n",
            "Input: [0.40639957 0.78998237 0.27988162 0.55390502 0.07239106 0.11237337\n",
            " 0.03220809 0.51004593 0.00605315 0.21017208], Target: [9], Predicted: 3\n",
            "Input: [0.86610547 0.75193711 0.93714271 0.40110661 0.99713656 0.45601704\n",
            " 0.91125767 0.24204572 0.615863   0.95682971], Target: [2], Predicted: 2\n",
            "Input: [0.52673771 0.03085474 0.97355107 0.27757082 0.37102387 0.80355966\n",
            " 0.52695995 0.27006636 0.85254258 0.19207372], Target: [9], Predicted: 4\n",
            "Input: [0.12770109 0.90311342 0.99512861 0.72169279 0.81371018 0.15874614\n",
            " 0.48758553 0.28703079 0.55219554 0.7476076 ], Target: [3], Predicted: 2\n",
            "Input: [0.21187776 0.99644161 0.58041847 0.81418034 0.85533644 0.25703947\n",
            " 0.04853645 0.11728767 0.21250249 0.48201007], Target: [2], Predicted: 2\n",
            "Input: [0.80057608 0.98640178 0.77846915 0.37214271 0.5738235  0.13276623\n",
            " 0.55996121 0.72801898 0.47825271 0.94492711], Target: [7], Predicted: 2\n",
            "Input: [0.80098566 0.04294188 0.34747065 0.6809994  0.03512272 0.2370475\n",
            " 0.721697   0.81515649 0.56450784 0.58377968], Target: [1], Predicted: 3\n",
            "Input: [0.03348631 0.11973567 0.51474742 0.68903077 0.15778423 0.83925125\n",
            " 0.72280241 0.76776816 0.99400587 0.12052859], Target: [8], Predicted: 8\n",
            "Input: [0.00276916 0.11277991 0.56383622 0.88743748 0.82928668 0.42404387\n",
            " 0.78056564 0.67580962 0.19465069 0.35247307], Target: [8], Predicted: 2\n",
            "Input: [0.66497144 0.88306431 0.85817767 0.14890686 0.90698144 0.70399101\n",
            " 0.51007829 0.3965272  0.63288796 0.24040914], Target: [2], Predicted: 2\n",
            "Input: [0.79666025 0.22908268 0.15846759 0.30850153 0.86564456 0.30114009\n",
            " 0.35461582 0.28355136 0.69729525 0.76912544], Target: [6], Predicted: 2\n",
            "Input: [0.82740993 0.99618573 0.95902198 0.83283419 0.65197714 0.10212732\n",
            " 0.86780749 0.61077364 0.76020993 0.23602647], Target: [4], Predicted: 2\n",
            "Input: [0.6855271  0.60901735 0.57513411 0.33673351 0.19884291 0.39330247\n",
            " 0.12852126 0.77947581 0.63863249 0.39894613], Target: [7], Predicted: 4\n",
            "Input: [0.6939378  0.3506945  0.13208589 0.23005591 0.28594293 0.47723159\n",
            " 0.49208908 0.3585608  0.31929697 0.74623317], Target: [7], Predicted: 2\n",
            "Input: [0.37145588 0.11641368 0.83507396 0.40819961 0.51938628 0.01283258\n",
            " 0.7026522  0.959388   0.64898507 0.9320204 ], Target: [1], Predicted: 2\n",
            "Input: [0.61057226 0.62404701 0.95978743 0.63704567 0.81058116 0.20258957\n",
            " 0.3771579  0.87872469 0.5744597  0.72502136], Target: [7], Predicted: 2\n",
            "Input: [0.75482724 0.68106335 0.46536895 0.42979678 0.42002641 0.55728588\n",
            " 0.1704933  0.32260652 0.13918627 0.49894697], Target: [4], Predicted: 2\n",
            "Input: [0.67561303 0.87008805 0.52062722 0.32538481 0.28075767 0.69757938\n",
            " 0.61755615 0.04067029 0.64894492 0.22527405], Target: [6], Predicted: 2\n",
            "Input: [0.40020147 0.35941731 0.33995851 0.48873264 0.39415579 0.63334754\n",
            " 0.99705658 0.40497777 0.6284775  0.70609629], Target: [4], Predicted: 2\n",
            "Input: [0.4844912  0.07997112 0.333522   0.5329343  0.45850011 0.68766901\n",
            " 0.11796357 0.65880032 0.66812615 0.48206786], Target: [4], Predicted: 4\n",
            "Input: [0.37670262 0.04641498 0.54787737 0.22654246 0.55128322 0.70389982\n",
            " 0.6250014  0.25797636 0.43154697 0.00519681], Target: [0], Predicted: 2\n",
            "Input: [0.25085546 0.57352401 0.37213113 0.14934137 0.97260216 0.25869991\n",
            " 0.49676765 0.37960908 0.77203649 0.86418125], Target: [4], Predicted: 2\n",
            "Input: [0.80699805 0.68262539 0.61273722 0.10417179 0.79891101 0.01855232\n",
            " 0.01218199 0.39981339 0.27563386 0.31417117], Target: [8], Predicted: 2\n",
            "Input: [0.76263677 0.52698691 0.82582136 0.41881329 0.57145796 0.21020098\n",
            " 0.88469909 0.99697563 0.96619711 0.10908341], Target: [3], Predicted: 3\n",
            "Input: [0.94334505 0.55950067 0.59725261 0.4158118  0.59971015 0.04946782\n",
            " 0.57000468 0.18292645 0.23040708 0.13306312], Target: [4], Predicted: 2\n",
            "Input: [0.53805209 0.35425487 0.50273433 0.73945187 0.13314058 0.13090332\n",
            " 0.00668772 0.2182849  0.91712672 0.71849161], Target: [4], Predicted: 4\n",
            "Input: [0.45146286 0.00550284 0.81816486 0.07556682 0.11531246 0.70885103\n",
            " 0.84466736 0.682957   0.80954533 0.1765631 ], Target: [8], Predicted: 2\n",
            "Input: [0.3903887  0.38550981 0.68154879 0.84329786 0.21613773 0.15924751\n",
            " 0.14452158 0.75514242 0.4381661  0.37318351], Target: [9], Predicted: 3\n",
            "Input: [0.91495639 0.64389253 0.61004364 0.9598117  0.63414299 0.9009593\n",
            " 0.55698177 0.46705777 0.58559921 0.94720621], Target: [0], Predicted: 2\n",
            "Input: [0.85667921 0.22417119 0.70292596 0.97733119 0.07638601 0.68111384\n",
            " 0.76242434 0.72820779 0.73817982 0.80147599], Target: [2], Predicted: 4\n",
            "Input: [0.97383363 0.07811387 0.92692921 0.38150262 0.99960769 0.86542265\n",
            " 0.2761016  0.50054532 0.94719425 0.63504428], Target: [1], Predicted: 4\n",
            "Input: [0.63584056 0.45162918 0.9724672  0.47098486 0.46060105 0.08057727\n",
            " 0.58502859 0.14443019 0.40398266 0.82068815], Target: [3], Predicted: 2\n",
            "Input: [0.00589583 0.15099253 0.47946096 0.48081769 0.53730688 0.59316343\n",
            " 0.05388333 0.54604218 0.63190192 0.06504083], Target: [3], Predicted: 3\n",
            "Input: [0.4203444  0.35652749 0.60953086 0.90992417 0.39126946 0.47783284\n",
            " 0.58224849 0.60478371 0.70479568 0.69244343], Target: [6], Predicted: 3\n",
            "Input: [0.63343895 0.02791357 0.34853563 0.60678956 0.55548778 0.27734778\n",
            " 0.56151948 0.9089779  0.36729451 0.23395741], Target: [4], Predicted: 3\n",
            "Input: [0.09566333 0.17972919 0.54391699 0.92839105 0.38171479 0.95884742\n",
            " 0.38822188 0.06924394 0.67669992 0.89957764], Target: [8], Predicted: 2\n",
            "Input: [0.07981251 0.87203464 0.43429634 0.93707021 0.42297095 0.88260938\n",
            " 0.93287076 0.85634585 0.44753544 0.29474099], Target: [3], Predicted: 2\n",
            "Input: [0.99019362 0.01005373 0.52502989 0.75837284 0.68887949 0.03271309\n",
            " 0.72825096 0.01399478 0.93229656 0.70926173], Target: [4], Predicted: 4\n",
            "Input: [0.48616056 0.39295564 0.42949447 0.66773841 0.89895235 0.27808318\n",
            " 0.94571064 0.23275418 0.27381775 0.17641482], Target: [1], Predicted: 2\n",
            "Input: [0.12640271 0.70143208 0.01838269 0.4340573  0.84999481 0.06571714\n",
            " 0.84655254 0.10454523 0.98862376 0.01010492], Target: [6], Predicted: 2\n",
            "Input: [0.45045935 0.9999731  0.30284721 0.37889011 0.89298745 0.79736102\n",
            " 0.12651089 0.32520536 0.63727851 0.29163639], Target: [3], Predicted: 2\n",
            "Input: [0.36206766 0.69041281 0.64741214 0.59949539 0.33139219 0.33048058\n",
            " 0.28532954 0.0239104  0.45643154 0.05656707], Target: [9], Predicted: 2\n",
            "Input: [0.25105278 0.74695295 0.04931579 0.30909503 0.74917423 0.57042587\n",
            " 0.67544647 0.76576131 0.80252539 0.44460217], Target: [2], Predicted: 2\n",
            "Input: [0.5088362  0.30218086 0.42489128 0.60191577 0.59014425 0.81626284\n",
            " 0.77319998 0.3308956  0.44010908 0.2720848 ], Target: [7], Predicted: 2\n",
            "Input: [0.21867558 0.85502913 0.14774434 0.19088672 0.13071782 0.28755316\n",
            " 0.76462606 0.44624854 0.00349091 0.02671547], Target: [7], Predicted: 2\n",
            "Input: [0.31911817 0.79588691 0.15594463 0.05526239 0.12294884 0.97607814\n",
            " 0.9847306  0.27570895 0.39495748 0.4615899 ], Target: [9], Predicted: 2\n",
            "Input: [0.68672189 0.30196045 0.21760099 0.48280378 0.79418626 0.80878645\n",
            " 0.82641439 0.64446963 0.23252495 0.71638083], Target: [8], Predicted: 2\n",
            "Input: [0.15829962 0.46723533 0.23453422 0.69924363 0.26543648 0.22760347\n",
            " 0.45765082 0.66536706 0.37527874 0.44292787], Target: [7], Predicted: 3\n",
            "Input: [0.55532994 0.18863288 0.89932284 0.90499468 0.71405325 0.91786131\n",
            " 0.66146459 0.94313282 0.21638951 0.31184863], Target: [0], Predicted: 2\n",
            "Input: [0.14172072 0.26760205 0.16927519 0.85592723 0.61108808 0.09947585\n",
            " 0.74450628 0.74125808 0.25658288 0.88437888], Target: [2], Predicted: 2\n",
            "Input: [0.40113108 0.68291061 0.9811115  0.5272691  0.82644179 0.67159908\n",
            " 0.7858611  0.98598734 0.3723313  0.18587555], Target: [1], Predicted: 2\n",
            "Input: [0.63930648 0.9073943  0.97490985 0.51227994 0.25110594 0.59070238\n",
            " 0.06258693 0.97209034 0.77491303 0.88445207], Target: [3], Predicted: 4\n",
            "Input: [0.44446086 0.14788468 0.88326497 0.38001912 0.48779221 0.50254028\n",
            " 0.71334479 0.84458622 0.28447844 0.09261117], Target: [9], Predicted: 2\n",
            "Input: [0.55169594 0.25255783 0.28289576 0.60321787 0.65602479 0.38586972\n",
            " 0.28849645 0.58855836 0.92940211 0.99069054], Target: [3], Predicted: 4\n",
            "Input: [0.97648826 0.69047282 0.73758308 0.4228419  0.31727506 0.68099146\n",
            " 0.92362241 0.28559617 0.30124586 0.32519319], Target: [7], Predicted: 2\n",
            "Input: [0.3432736  0.80979468 0.44836141 0.04881017 0.70093835 0.92942506\n",
            " 0.92591409 0.21204251 0.68892343 0.39831229], Target: [7], Predicted: 2\n",
            "Input: [0.29692561 0.62593692 0.62024891 0.48703002 0.07170248 0.38004815\n",
            " 0.92405031 0.18242738 0.56437332 0.30741823], Target: [2], Predicted: 2\n",
            "Input: [0.77316113 0.19218843 0.80127726 0.98378175 0.72494372 0.15761827\n",
            " 0.39359873 0.88782952 0.16595637 0.29723679], Target: [9], Predicted: 3\n",
            "Input: [0.81201845 0.69847465 0.19002228 0.10251726 0.02707757 0.36123944\n",
            " 0.74223535 0.1822358  0.54200055 0.10511306], Target: [8], Predicted: 4\n"
          ]
        }
      ]
    }
  ]
}